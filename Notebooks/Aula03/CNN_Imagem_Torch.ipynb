{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checando se a gpu está disponível senão roda na cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prática\n",
    "1. Aquisição e pré-processamento dos dados\n",
    "2. Treinamento\n",
    "* Implementar arquitetura\n",
    "* Definir otimizadores, métricas e regularizadores\n",
    "3. Teste (avaliação de desempenho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Aquisição dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:19<00:00, 518694.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/train-images-idx3-ubyte.gz to MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 85487433.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:02<00:00, 570272.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 4013172.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transforms = transforms.Compose([\n",
    "                                 transforms.ToTensor()\n",
    "])\n",
    " \n",
    "# Loading Data and splitting it into train and validation data\n",
    "train = datasets.MNIST('', train = True, transform = transforms, download = True)\n",
    "train, valid = random_split(train,[50000,10000])\n",
    " \n",
    "# Create Dataloader of the above tensor with batch size = 32\n",
    "trainloader = DataLoader(train, batch_size=32)\n",
    "validloader = DataLoader(valid, batch_size=32)\n",
    "\n",
    "testset = datasets.MNIST('', train=False,\n",
    "                                       download=True, transform=transforms)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "                                         shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1 , 3)\n",
    "        self.fc1 = nn.Linear(676, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "min_valid_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t\t Training Loss: 0.5106308129988492 \t\t Validation Loss: 0.308134754113972\n",
      "Validation Loss Decreased(inf--->96.44617803767323) \t Saving The Model\n",
      "Epoch 2 \t\t Training Loss: 0.2821932460145068 \t\t Validation Loss: 0.2716877277428731\n",
      "Validation Loss Decreased(96.44617803767323--->85.03825878351927) \t Saving The Model\n",
      "Epoch 3 \t\t Training Loss: 0.24846512832400588 \t\t Validation Loss: 0.24348129796239135\n",
      "Validation Loss Decreased(85.03825878351927--->76.20964626222849) \t Saving The Model\n",
      "Epoch 4 \t\t Training Loss: 0.228678022504749 \t\t Validation Loss: 0.22215326474949765\n",
      "Validation Loss Decreased(76.20964626222849--->69.53397186659276) \t Saving The Model\n",
      "Epoch 5 \t\t Training Loss: 0.21493896128108067 \t\t Validation Loss: 0.20960940319461563\n",
      "Validation Loss Decreased(69.53397186659276--->65.6077431999147) \t Saving The Model\n",
      "Epoch 6 \t\t Training Loss: 0.20387803963716222 \t\t Validation Loss: 0.20201123752794897\n",
      "Validation Loss Decreased(65.6077431999147--->63.22951734624803) \t Saving The Model\n",
      "Epoch 7 \t\t Training Loss: 0.19474751204168822 \t\t Validation Loss: 0.1984623995291206\n",
      "Validation Loss Decreased(63.22951734624803--->62.11873105261475) \t Saving The Model\n",
      "Epoch 8 \t\t Training Loss: 0.1872761760226386 \t\t Validation Loss: 0.1938276066632269\n",
      "Validation Loss Decreased(62.11873105261475--->60.66804088559002) \t Saving The Model\n",
      "Epoch 9 \t\t Training Loss: 0.18059285857934584 \t\t Validation Loss: 0.19042660233478387\n",
      "Validation Loss Decreased(60.66804088559002--->59.60352653078735) \t Saving The Model\n",
      "Epoch 10 \t\t Training Loss: 0.17475452535009334 \t\t Validation Loss: 0.18709164160116318\n",
      "Validation Loss Decreased(59.60352653078735--->58.55968382116407) \t Saving The Model\n",
      "Epoch 11 \t\t Training Loss: 0.1695537402625872 \t\t Validation Loss: 0.18416345162323108\n",
      "Validation Loss Decreased(58.55968382116407--->57.64316035807133) \t Saving The Model\n",
      "Epoch 12 \t\t Training Loss: 0.16378664006094756 \t\t Validation Loss: 0.18043510373622274\n",
      "Validation Loss Decreased(57.64316035807133--->56.47618746943772) \t Saving The Model\n",
      "Epoch 13 \t\t Training Loss: 0.15638642794627194 \t\t Validation Loss: 0.1774821426011265\n",
      "Validation Loss Decreased(56.47618746943772--->55.55191063415259) \t Saving The Model\n",
      "Epoch 14 \t\t Training Loss: 0.15009477685616182 \t\t Validation Loss: 0.17508718712677876\n",
      "Validation Loss Decreased(55.55191063415259--->54.80228957068175) \t Saving The Model\n",
      "Epoch 15 \t\t Training Loss: 0.1448503479105912 \t\t Validation Loss: 0.17394152665421508\n",
      "Validation Loss Decreased(54.80228957068175--->54.443697842769325) \t Saving The Model\n",
      "Epoch 16 \t\t Training Loss: 0.1405055445216129 \t\t Validation Loss: 0.17223757935670048\n",
      "Validation Loss Decreased(54.443697842769325--->53.910362338647246) \t Saving The Model\n",
      "Epoch 17 \t\t Training Loss: 0.13638438704117933 \t\t Validation Loss: 0.1717221387468588\n",
      "Validation Loss Decreased(53.910362338647246--->53.7490294277668) \t Saving The Model\n",
      "Epoch 18 \t\t Training Loss: 0.1326830869040649 \t\t Validation Loss: 0.17151903318532835\n",
      "Validation Loss Decreased(53.7490294277668--->53.68545738700777) \t Saving The Model\n",
      "Epoch 19 \t\t Training Loss: 0.129128503848537 \t\t Validation Loss: 0.1714601491253597\n",
      "Validation Loss Decreased(53.68545738700777--->53.66702667623758) \t Saving The Model\n",
      "Epoch 20 \t\t Training Loss: 0.125790762370637 \t\t Validation Loss: 0.17304039805437238\n",
      "Epoch 21 \t\t Training Loss: 0.12264896834969892 \t\t Validation Loss: 0.17377832237357338\n",
      "Epoch 22 \t\t Training Loss: 0.11901648462912687 \t\t Validation Loss: 0.1732983222162833\n",
      "Epoch 23 \t\t Training Loss: 0.11512584189058508 \t\t Validation Loss: 0.17378680962033735\n",
      "Epoch 24 \t\t Training Loss: 0.11161508096110312 \t\t Validation Loss: 0.1734018570925005\n",
      "Epoch 25 \t\t Training Loss: 0.10812776118769565 \t\t Validation Loss: 0.17437445212262698\n",
      "Epoch 26 \t\t Training Loss: 0.10502166813194766 \t\t Validation Loss: 0.17562191934137347\n",
      "Epoch 27 \t\t Training Loss: 0.10216606687262812 \t\t Validation Loss: 0.1763187891464478\n",
      "Epoch 28 \t\t Training Loss: 0.09942984136395866 \t\t Validation Loss: 0.17881800379297627\n",
      "Epoch 29 \t\t Training Loss: 0.09667520144361745 \t\t Validation Loss: 0.18009530874817206\n",
      "Epoch 30 \t\t Training Loss: 0.09443864485024583 \t\t Validation Loss: 0.1824884062860542\n",
      "Epoch 31 \t\t Training Loss: 0.0919272014047469 \t\t Validation Loss: 0.18474322837218046\n",
      "Epoch 32 \t\t Training Loss: 0.08949716426047806 \t\t Validation Loss: 0.187186370272719\n",
      "Epoch 33 \t\t Training Loss: 0.08682059585207672 \t\t Validation Loss: 0.18838596272210892\n",
      "Epoch 34 \t\t Training Loss: 0.08436395444904865 \t\t Validation Loss: 0.19101427403983837\n",
      "Epoch 35 \t\t Training Loss: 0.08186804683270053 \t\t Validation Loss: 0.1932793100365994\n",
      "Epoch 36 \t\t Training Loss: 0.07926749477254719 \t\t Validation Loss: 0.19498485115396913\n",
      "Epoch 37 \t\t Training Loss: 0.07677094548373403 \t\t Validation Loss: 0.19912817317638368\n",
      "Epoch 38 \t\t Training Loss: 0.0745778372514247 \t\t Validation Loss: 0.2010815760601288\n",
      "Epoch 39 \t\t Training Loss: 0.07199492445007033 \t\t Validation Loss: 0.20464068652017595\n",
      "Epoch 40 \t\t Training Loss: 0.07010622158661778 \t\t Validation Loss: 0.2091057700485872\n",
      "Epoch 41 \t\t Training Loss: 0.06787419001181846 \t\t Validation Loss: 0.2121957053989023\n",
      "Epoch 42 \t\t Training Loss: 0.06613475191771122 \t\t Validation Loss: 0.2151137570163961\n",
      "Epoch 43 \t\t Training Loss: 0.06431682888126913 \t\t Validation Loss: 0.2176646433767681\n",
      "Epoch 44 \t\t Training Loss: 0.06258131715305053 \t\t Validation Loss: 0.22324160102809879\n",
      "Epoch 45 \t\t Training Loss: 0.0606393332132323 \t\t Validation Loss: 0.22709079445213282\n",
      "Epoch 46 \t\t Training Loss: 0.05915524750356267 \t\t Validation Loss: 0.23192227391463618\n",
      "Epoch 47 \t\t Training Loss: 0.05740260841370806 \t\t Validation Loss: 0.23605413977767137\n",
      "Epoch 48 \t\t Training Loss: 0.055850561007262066 \t\t Validation Loss: 0.24080377682458057\n",
      "Epoch 49 \t\t Training Loss: 0.05425461915379968 \t\t Validation Loss: 0.24741196063121768\n",
      "Epoch 50 \t\t Training Loss: 0.052769708839641084 \t\t Validation Loss: 0.2522080355352118\n",
      "Epoch 51 \t\t Training Loss: 0.05112437368224422 \t\t Validation Loss: 0.25837972466828313\n",
      "Epoch 52 \t\t Training Loss: 0.04986514603794044 \t\t Validation Loss: 0.26410366882631087\n",
      "Epoch 53 \t\t Training Loss: 0.04853231124597782 \t\t Validation Loss: 0.2730235264214152\n",
      "Epoch 54 \t\t Training Loss: 0.047416020349611857 \t\t Validation Loss: 0.27777938890462806\n",
      "Epoch 55 \t\t Training Loss: 0.04610498079014559 \t\t Validation Loss: 0.2873513600808072\n",
      "Epoch 56 \t\t Training Loss: 0.04475036330513156 \t\t Validation Loss: 0.29262741055535213\n",
      "Epoch 57 \t\t Training Loss: 0.04369809068811848 \t\t Validation Loss: 0.3011260716813498\n",
      "Epoch 58 \t\t Training Loss: 0.042842523780035724 \t\t Validation Loss: 0.30502995593834464\n",
      "Epoch 59 \t\t Training Loss: 0.041406880806441004 \t\t Validation Loss: 0.31505713012564973\n",
      "Epoch 60 \t\t Training Loss: 0.04056843326502493 \t\t Validation Loss: 0.32243950128496346\n",
      "Epoch 61 \t\t Training Loss: 0.03943544019868454 \t\t Validation Loss: 0.32848026759874804\n",
      "Epoch 62 \t\t Training Loss: 0.03822146022896273 \t\t Validation Loss: 0.33324940265517106\n",
      "Epoch 63 \t\t Training Loss: 0.037518743921420204 \t\t Validation Loss: 0.34058849316983403\n",
      "Epoch 64 \t\t Training Loss: 0.03599280831155633 \t\t Validation Loss: 0.34552495690101137\n",
      "Epoch 65 \t\t Training Loss: 0.03573237630799683 \t\t Validation Loss: 0.354227110055718\n",
      "Epoch 66 \t\t Training Loss: 0.03434830757527337 \t\t Validation Loss: 0.35874570533570466\n",
      "Epoch 67 \t\t Training Loss: 0.03241431888762297 \t\t Validation Loss: 0.36454074072851644\n",
      "Epoch 68 \t\t Training Loss: 0.032299822455563995 \t\t Validation Loss: 0.3723416146169907\n",
      "Epoch 69 \t\t Training Loss: 0.03131506130717411 \t\t Validation Loss: 0.38332197673892004\n",
      "Epoch 70 \t\t Training Loss: 0.030174321199748846 \t\t Validation Loss: 0.3785649910567157\n",
      "Epoch 71 \t\t Training Loss: 0.028973230880906982 \t\t Validation Loss: 0.39946666702175665\n",
      "Epoch 72 \t\t Training Loss: 0.028197231061134573 \t\t Validation Loss: 0.4018030191891274\n",
      "Epoch 73 \t\t Training Loss: 0.028154967443942944 \t\t Validation Loss: 0.4233782090356883\n",
      "Epoch 74 \t\t Training Loss: 0.028479235184673535 \t\t Validation Loss: 0.41029378104761993\n",
      "Epoch 75 \t\t Training Loss: 0.024736574562803974 \t\t Validation Loss: 0.4141540303631321\n",
      "Epoch 76 \t\t Training Loss: 0.026053572374740867 \t\t Validation Loss: 0.42684347343281165\n",
      "Epoch 77 \t\t Training Loss: 0.025577780617948114 \t\t Validation Loss: 0.4326099017167862\n",
      "Epoch 78 \t\t Training Loss: 0.025026247484681515 \t\t Validation Loss: 0.41924413898506496\n",
      "Epoch 79 \t\t Training Loss: 0.022903576040437294 \t\t Validation Loss: 0.426701950923967\n",
      "Epoch 80 \t\t Training Loss: 0.023117090966097143 \t\t Validation Loss: 0.44774044597040075\n",
      "Epoch 81 \t\t Training Loss: 0.023661767803241557 \t\t Validation Loss: 0.4416168639820633\n",
      "Epoch 82 \t\t Training Loss: 0.021935007556127584 \t\t Validation Loss: 0.4562523586989174\n",
      "Epoch 83 \t\t Training Loss: 0.022216046327547635 \t\t Validation Loss: 0.46100367530538827\n",
      "Epoch 84 \t\t Training Loss: 0.021790414803688846 \t\t Validation Loss: 0.4794793990110866\n",
      "Epoch 85 \t\t Training Loss: 0.02080294455165741 \t\t Validation Loss: 0.47366717740794756\n",
      "Epoch 86 \t\t Training Loss: 0.020583546639505946 \t\t Validation Loss: 0.47744145566020463\n",
      "Epoch 87 \t\t Training Loss: 0.02031023535600244 \t\t Validation Loss: 0.47482708952561287\n",
      "Epoch 88 \t\t Training Loss: 0.020285658106246874 \t\t Validation Loss: 0.49182313200085914\n",
      "Epoch 89 \t\t Training Loss: 0.019099922294974838 \t\t Validation Loss: 0.49806309661943904\n",
      "Epoch 90 \t\t Training Loss: 0.01792051726839611 \t\t Validation Loss: 0.5107547575596207\n",
      "Epoch 91 \t\t Training Loss: 0.01927378576630601 \t\t Validation Loss: 0.5094512876792404\n",
      "Epoch 92 \t\t Training Loss: 0.01930590139825477 \t\t Validation Loss: 0.5207439057930486\n",
      "Epoch 93 \t\t Training Loss: 0.018361728237861072 \t\t Validation Loss: 0.5118738633276165\n",
      "Epoch 94 \t\t Training Loss: 0.018048562634925696 \t\t Validation Loss: 0.53354679115329\n",
      "Epoch 95 \t\t Training Loss: 0.01717077370136069 \t\t Validation Loss: 0.5341541383402162\n",
      "Epoch 96 \t\t Training Loss: 0.016736760813439708 \t\t Validation Loss: 0.5494751098650296\n",
      "Epoch 97 \t\t Training Loss: 0.01630167828137217 \t\t Validation Loss: 0.5642821230282488\n",
      "Epoch 98 \t\t Training Loss: 0.01787661347783276 \t\t Validation Loss: 0.5452479216919771\n",
      "Epoch 99 \t\t Training Loss: 0.014961951508257647 \t\t Validation Loss: 0.5701017263561062\n",
      "Epoch 100 \t\t Training Loss: 0.015945406205443616 \t\t Validation Loss: 0.5676387461194203\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    train_loss = 0.0\n",
    "    for data, labels in trainloader:\n",
    "        # Transfer Data to GPU if available\n",
    "        # if torch.cuda.is_available():\n",
    "        #     data, labels = data.cuda(), labels.cuda()\n",
    "         \n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward Pass\n",
    "        target = net(data)\n",
    "        # Find the Loss\n",
    "        loss = criterion(target,labels)\n",
    "        # Calculate gradients \n",
    "        loss.backward()\n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "        # Calculate Loss\n",
    "        train_loss += loss.item()\n",
    "     \n",
    "    valid_loss = 0.0\n",
    "    net.eval()     # Optional when not using Model Specific layer\n",
    "    for data, labels in validloader:\n",
    "        # Transfer Data to GPU if available\n",
    "        # if torch.cuda.is_available():\n",
    "        #     data, labels = data.cuda(), labels.cuda()\n",
    "         \n",
    "        # Forward Pass\n",
    "        target = net(data)\n",
    "        # Find the Loss\n",
    "        loss = criterion(target,labels)\n",
    "        # Calculate Loss\n",
    "        valid_loss += loss.item()\n",
    "    training_loss_print = train_loss / len(trainloader)\n",
    "    valid_loss_print = valid_loss / len(validloader)\n",
    "    print(f'Epoch {epoch+1} \\t\\t Training Loss: {training_loss_print} \\t\\t Validation Loss: {valid_loss_print}')\n",
    "     \n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss}--->{valid_loss}) \\t Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "         \n",
    "        # Saving State Dict\n",
    "        torch.save(net.state_dict(), 'saved_model.pth')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90625"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "accuracy_score(labels, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valor_venal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
