{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TITANIC DATASET - ML - RANDOM FOREST\n",
    "===================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import mlflow\n",
    "import pickle\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from functions import get_metrics\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_TITLE = \"Titanic Dataset Analyzes\"\n",
    "\n",
    "run_description = \"\"\"\n",
    "### Descrição\n",
    "\n",
    "Implementação usando [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\"\"\"\n",
    "\n",
    "tags = {\n",
    "    \"date\": datetime.now(),\n",
    "    \"author\": \"Pablo Veinberg\",\n",
    "    \"version\": 1.0,\n",
    "    \"envoronment\": \"local\",\n",
    "    \"mlflow.note.content\": run_description,\n",
    "    \"mlflow.runName\": \"Random Forest\",\n",
    "    \"data_source\": \"./../datasets/silver/train-encoded-not-normalize.parquet\",\n",
    "    \"train_test_dataset\": \"./../datasets/silver/titanic-train-test-data.pkl\"\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"token\": np.random.randint(10_000,high=99_000),\n",
    "    \"criterion\": \"entropy\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo_veinberg/.local/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:143: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_parquet(tags['data_source'])\n",
    "mlflow_dataset = mlflow.data.from_pandas(dataset, \\\n",
    "                                            source=tags['data_source'], \\\n",
    "                                            name=\"Titanic Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712, 8), (179, 8), (712,), (179,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./../datasets/silver/titanic-train-test-data.pkl', 'rb') as file:\n",
    "    X_train, X_test, y_train, y_test = pickle.load(file)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo_veinberg/.local/lib/python3.10/site-packages/mlflow/data/pandas_dataset.py:134: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  return _infer_schema(self._df)\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(EXPERIMENT_TITLE)\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Settings\n",
    "    mlflow.log_input(mlflow_dataset)\n",
    "    mlflow.set_tags(tags)\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    model = RandomForestClassifier(criterion=params[\"criterion\"])\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    plot_confusion_matrix_path = f\"./../results/random_forest_{params['token']}.png\"\n",
    "    metrics = get_metrics(y_test, y_pred, plot_confusion_matrix_path, \"Matrix Confusion - Titanic Dataset\")\n",
    "    \n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.log_artifact(plot_confusion_matrix_path)\n",
    "    \n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[102,   3],\n",
       "       [  7,  67]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Survival|0 = No, 1 = Yes\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O modelo é bom em 93.58% para acertar não sobreviventes\n",
      "Quando acerta, está correto num 97.14%\n",
      "Esta informação foi baseada em 105 registros\n",
      "==================================================\n",
      "O modelo é bom em 95.71% para acertar sobreviventes\n",
      "Quando acerta, está correto num 90.54%\n",
      "Esta informação foi baseada em 74 registros\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "class_report = classification_report(y_test, y_pred, \n",
    "                           output_dict=True)\n",
    "\n",
    "results = class_report['macro avg']\n",
    "\n",
    "\n",
    "# precisão indica quão bom o modelo é para detectar positivos\n",
    "\n",
    "labels = ['não sobreviventes', 'sobreviventes']\n",
    "\n",
    "for key, value in enumerate(labels):\n",
    "    print(f\"O modelo é bom em {round(class_report[str(key)]['precision'] * 100 ,2)}% para acertar {value}\")\n",
    "    print(f\"Quando acerta, está correto num {round(class_report[str(key)]['recall'] * 100 ,2)}%\")\n",
    "    print(f\"Esta informação foi baseada em {round(class_report[str(key)]['support'],0)} registros\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# print(f\"O modelo é bom em {round(class_report['0']['precision'] * 100 ,2)}% para acertar não sobreviventes\")\n",
    "# print(f\"Quando acerta, está correto num {round(class_report['0']['recall'] * 100 ,2)}%\")\n",
    "\n",
    "\n",
    "# # recall indica quando a resposta real é sim, quanto o modelo acertou\n",
    "# print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95       105\n",
      "           1       0.96      0.91      0.93        74\n",
      "\n",
      "    accuracy                           0.94       179\n",
      "   macro avg       0.95      0.94      0.94       179\n",
      "weighted avg       0.94      0.94      0.94       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169, 10, 0.9441340782122905)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
